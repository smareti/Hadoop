Benchmarking algorithms:
-------------------------
host DEV 01 server:hostname
location:/tmp/temp/hive/hive-testbench/sample-queries-tpcds

sample spark SQL query:

spark-sql --master yarn --conf "spark.dynamicAllocation.enabled=true" --conf "spark.shuffle.service.enabled=true" --conf "spark.executor.memory=16gb" --conf "spark.executor.cores=4" --conf "hive.exec.orc.split.strategy=BI" --queue default

export SPARK_MAJOR_VERSION=2
spark-sql --queue default --master yarn --conf "spark.dynamicAllocation.enabled=true" --conf "spark.shuffle.service.enabled=true" --conf "spark.executor.memory=16gb" --conf "spark.executor.cores=4" --conf "hive.exec.orc.split.strategy=BI" --conf "spark.eventLog.enabled=true" --conf "spark.unravel.workflow.name=Spark_query19" --conf "spark.unravel.workflow.utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ')"  -f /tmp/temp/hive/hive-testbench/sample-queries-tpcds/spark_sample_job.sh > /tmp/temp/hive/hive-testbench/sample-queries-tpcds/spark_sample_job.log 2>&1


sample hive query:

jdbc:hive2://hostname.corp..com:2181,hostname.corp..com:2181,paz1hmndl02p.corp..com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2

beeline -u "jdbc:hive2://hostname.corp..com:10000/tpcds_bin_partitioned_orc_100;principal=hive/_HOST@CORP.CVS.COM?tez.queue.name=default" -f ${query}.sql >./out/${query}.out

beeline -u "jdbc:hive2://hostname.corp.cvscaremark.com:2181,hostname.corp..com:2181,hostname.corp..com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2" --hiveconf unravel.workflow.name="Hive_query19"   --hiveconf unravel.workflow.utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ')  -f /tmp/temp/hive/hive-testbench/sample-queries-tpcds/hive_sample_job.sql > /tmp/temp/hive/hive-testbench/sample-queries-tpcds/out/hive_sample_job.sql.log


pyspark sql:

spark-submit --master yarn --driver-memory 20G --num-executors 200 --executor-memory 32G --executor-cores 5 pyspark_sample_job.py

export SPARK_MAJOR_VERSION=2
spark-submit --deploy-mode client --master yarn  --driver-memory 2G --num-executors 20 --executor-memory 10G --executor-cores 5 --conf "spark.unravel.workflow.name=pyspark_query19" --conf "spark.unravel.workflow.utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ')"  --conf "spark.eventLog.enabled=true"  --files /usr/hdp/current/spark2-client/conf/hive-site.xml pyspark_sample_job.py >  pyspark_sample_job.py.log


sc = SparkContext.getOrCreate()
hive_context = HiveContext(sc)
sqlContext = SQLContext(sc)
sqlContext.setConf("spark.sql.orc.filterPushdown","true")
sqlContext.setConf("spark.sql.shuffle.partitions",200)
sqlContext.setConf("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

https://unraveldata.atlassian.net/wiki/spaces/UN43/pages/223641673/Tagging+Workflows#TaggingWorkflows-TagMapReduce

sample jobs
--------------
vi hive_sample_job.sql
-- start query 1 in stream 0 using template query19.tpl and seed 1930872976
set tez.queue.name=default;
use tpcds_bin_partitioned_orc_100;

select  i_brand_id brand_id, i_brand brand, i_manufact_id, i_manufact,
        sum(ss_ext_sales_price) ext_price
 from date_dim, store_sales, item,customer,customer_address,store
 where d_date_sk = ss_sold_date_sk
   and ss_item_sk = i_item_sk
   and i_manager_id=7
   and d_moy=11
   and d_year=1999
   and ss_customer_sk = c_customer_sk
   and c_current_addr_sk = ca_address_sk
   and substr(ca_zip,1,5) <> substr(s_zip,1,5)
   and ss_store_sk = s_store_sk
 group by i_brand
      ,i_brand_id
      ,i_manufact_id
      ,i_manufact
 order by ext_price desc
         ,i_brand
         ,i_brand_id
         ,i_manufact_id
         ,i_manufact
limit 100 ;

-- end query 1 in stream 0 using template query19.tpl

vi spark_sample_job.sh
export SPARK_MAJOR_VERSION=2

spark-sql --queue default --master yarn --conf "spark.dynamicAllocation.enabled=true" --conf "spark.shuffle.service.enabled=true" --conf "spark.executor.memory=16gb" --conf "spark.executor.cores=4" --conf "hive.exec.orc.split.strategy=BI" --conf "spark.eventLog.enabled=true" --conf "spark.unravel.workflow.name=Spark_query19" --conf "spark.unravel.workflow.utctimestamp=$(date -u '+%Y%m%dT%H%M%SZ')"  -f query19.sql > query19.sql.log 2>&1

vi pyspark_sample_job.py
#! /bin/python

from pyspark.sql import SQLContext
#import sqlContext.implicits._
#from pyspark.sql.functions import concat, col, lit, count, when
#from pyspark.sql import functions as F
#from pyspark.sql.functions import unix_timestamp
#from pyspark.sql.functions import udf
from pyspark.sql.types import *
#from pyspark.sql.functions import lit
from pyspark.sql import HiveContext
from pyspark import SparkConf, SparkContext
#from pyspark.sql.functions import concat, col, lit, count, when
#from pyspark.sql.functions import countDistinct
#import math
#import re
#import copy
#import time
import datetime
from pyspark import SparkContext
from pyspark import HiveContext
from pyspark import SparkContext,SparkConf


sc = SparkContext.getOrCreate()
hive_context = HiveContext(sc)
sqlContext = SQLContext(sc)
sqlContext.setConf("spark.sql.orc.filterPushdown","true")
sqlContext.setConf("spark.sql.shuffle.partitions",200)
sqlContext.setConf("spark.serializer", "org.apache.spark.serializer.KryoSerializer")


#spark.conf.set("spark.sql.shuffle.partitions", 10)
#SparkSession.builder.enableHiveSupport().getOrCreate()
#sc =SparkContext()
#sqlContext = SQLContext(sc)
#df1 = sqlContext.sql(""" select * from tpcds_bin_partitioned_orc_100.store_sales """)
#df1 = sqlContext.sql(""" create table tpcds_bin_partitioned_orc_100.test1 as select * from  tpcds_bin_partitioned_orc_100.store_sales limit 5 """)
sqlContext.sql("show databases").show()



tpc becnhmark on hadoop
------------------------
host: paz1hendl01p
yum install git
yum install gcc*
cd /tmp/temp/hive/
git clone https://github.com/hortonworks/hive-testbench
cd /tmp/temp/hive/hive-testbench/

Prepare Hive Tables:
[hive@rri2hdpedl2p hive-testbench-hive14]$ locate javac | grep jdk
/usr/jdk64/jdk1.8.0_40/bin/javac

Change tpcds-build.sh to point to javac:
#!/bin/sh

export PATH="/usr/jdk64/jdk1.8.0_40/bin/:$PATH"
./tpcds-build.sh

vi tpcds-setup.sh
HIVE='beeline -u "jdbc:hive2://hostname.corp..com:10000/default;principal=hive/_HOST@CORP.CVS.COM?tez.queue.name=default" --hiveconf hive.exec.dynamic.partition.mode=nonstrict --hiveconf hive.exec.max.dynamic.partitions.pernode=100000 --hiveconf hive.exec.max.dynamic.partitions=100000 --hiveconf hive.exec.max.created.files=1000000 --hiveconf hive.exec.parallel=true --hiveconf hive.stats.autogather=false --hiveconf hive.optimize.sort.dynamic.partition=true --hiveconf hive.optimize.sort.dynamic.partition=true --hiveconf tez.runtime.empty.partitions.info-via-events.enabled=true --hiveconf hive.tez.auto.reducer.parallelism=true --hiveconf hive.tez.min.partition.factor=0.01'

runcommand "$HIVE  -f ddl-tpcds/text/alltables.sql --hivevar DB=tpcds_text_${SCALE} --hivevar LOCATION=${DIR}/${SCALE}"
./tpcds-setup.sh 2 (2 GB)
cd sample-queries-tpcds
vi run_all_cvs_queries.sh
for i in 19 42 52 55 63 68 73 98 3 7 27 43 53 89 34 46 59 79 96 48
do
 query=query${i}
echo $(date) "Started query ${query}"
beeline -u "jdbc:hive2://hostname.corp..com:10000/tpcds_bin_partitioned_orc_100;principal=hive/_HOST@CORP.CVS.COM?tez.queue.name=default" -f ${query}.sql >./out/${query}.out | tee ./out/${query}.sysout
echo $(date) "ended query ${query}"
done
sh run_all_cvs_queries.sh

HiBench 
-------
host: hosname
cd /tmp/temp/hive
git clone https://github.com/intel-hadoop/HiBench
cd HiBench/
cd /tmp/temp/hive/HiBench/conf
cp hadoop.conf.template hadoop.conf
vi hadoop.conf
# Hadoop home
hibench.hadoop.home     /usr/hdp/current/hadoop-client

# The path of hadoop executable
hibench.hadoop.executable     ${hibench.hadoop.home}/bin/hadoop

# Hadoop configraution directory
hibench.hadoop.configure.dir  ${hibench.hadoop.home}/etc/hadoop

# The root HDFS path to store HiBench data
hibench.hdfs.master       hdfs://hostname.corp.cvscaremark.com:8020


# Hadoop release provider. Supported value: apache, cdh5, hdp
hibench.hadoop.release    hdp
hibench.hadoop.mapreduce.home /usr/hdp/current/hadoop-mapreduce-client

vi hibench.conf
hibench.scale.profile                 bigdata
