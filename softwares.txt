sudo du -ha /var/log/| sort -rh | head -15

sudo view
:!/bin/bash
 

klist -kt /etc/security/keytabs/hdfs.headless.keytab
kinit -kt /etc/security/keytabs/hdfs.headless.keytab hdfs-DEV_PBM@TEST..COM
klist -l


beeline -u "jdbc:hive2://hostname...com:2181,...com:2181,...com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-http"

for host in `cat hosts`; do echo $host; ssh -q hdpadm@$host 'id cmsadbatch' ; done
yarn application -movetoqueue application_1535871991734_1848 -queue pa

Two things in islon: map AD/lamin1  to Linux shel id lamin1  and also have user lamin1  added to hdfs proxy

beeline -u "jdbc:hive2://hostname...com:10000/default;transportMode=binary;principal=hive/...com@..COM;auth=kerberos;saslQop=auth-conf"

beeline -u "jdbc:hive2://hostanme...com:10000/default;transportMode=binary;principal=hive/...com@..COM;auth=kerberos;saslQop=auth-conf"

1)	Beeline with Zookeeper URL  ( "jdbc:hive2://hostname...com:2181,...com:2181,...com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-http" ) 

2)	Normal connection string (beeline -u "jdbc:hive2://hostname...com:10000/default;principal=hive/_HOST@TEST..COM")

Could you  please execute the following commands in RTL Dev and RTL UAT zone for providing HDFS access to the project group.
RTL DEV
chmod -R +a group 2025 allow dir_gen_all,container_inherit,object_inherit  /data/dev/MINCLI

$JAVA_HOME/bin/jmap -histo `cat /var/run/hive/hive-server.pid` > /tmp/histo.txt $JAVA_HOME/bin/jmap -heap `cat /var/run/hive/hive-server.pid` > /tmp/heap.txt 
$JAVA_HOME/bin/jmap -dump:file=/tmp/hs2.hprof `cat /var/run/hive/hive-server.pid`    (please make sure that /tmp has sufficient space to collect the heap size)

[hdfs@hostname ~]$ curl -sS -L -w '%{http_code}' -X PUT --negotiate -u : 'http://hostanem,e...com:8082/webhdfs/v1/hdp/apps/2.6.2.0-205/sqoop?op=SETPERMISSION&permission=555'

for host in `cat known_hosts`; do echo $host;  scp /home/hadoop/anaconda2/Anaconda2-4.3.1-Linux-x86_64.sh root@$host:/root; done
for host in `cat known_hosts`; do echo $host;  ssh -q root@$host 'bash /root/Anaconda2-4.3.1-Linux-x86_64.sh -b -p /home/hadoop/anaconda2'; done
for host in `cat known_hosts`; do echo $host;  ssh -q root@$host 'rm -f Anaconda2-4.3.1-Linux-x86_64.sh'; done

ps huH p <PID_OF_U_PROCESS> | wc -l 

jdbc:hive2://hostname...com:10000/default;principal=hive/...com@TEST..COM?hive.insert.into.multilevel.dirs=true;hive.enforce.bucketing=true;mapreduce.map.memory.mb=12000;mapreduce.map.java.opts=-Xmx9600m;hive.exec.orc.split.strategy=BI;

for host in `cat hosts`; do echo $host;  ssh -q root@$host 'echo -e "Downtown123$\nDowntown123$" | passwd z247595'; done 

Try running this command prior to the query: 
set hive.tez.container.size=8192;
This command will double the TEZ container size and may prevent the out of memory exception.

curl -k -u "z247595:Downtown123$" -H "X-Requested-By: ambari" -X GET  https://hostname:8080/api/v1/clusters/DEV_PBM/hosts

curl -k -u "z247595:Downtown123$" -H "X-Requested-By: ambari" -X GET https://hostname:8080/api/v1/clusters/DEV_PBM/hosts | grep -E host_name |awk -v FS="\"host_name\" :" 'NF>1{print $2}'|sed  's/"//g' |grep -v hostname...com > hosts.txt

keytool - -list - -keystore “keystore_name.jks” - -verbose  

for i in {1..10} ; do /usr/jdk64/jdk1.8.0_40/bin/jstack 12308 > stack_$i; sleep 5s; done

[hdpadm@hostname ~]$ /usr/bin/ldapsearch -x -W -H ldap://...com:389 -b "CN=Users,DC=,DC=,DC=com" -D "vi_kadmin_ambari" -s one samaccountname=z247595

keytab command
------------
ktutil <<EOF
add_entry -password -p unravel/hostname@.COM -k 1 -e arcfour-hmac
hadoopRocks123!
write_kt /etc/security/keytabs/unravel.keytab
list
exit
EOF

sparl smaple job
spark-submit --class org.apache.spark.examples.SparkPi   --master yarn-client  --num-executors 1  --driver-memory 512m --executor-memory 512m  --executor-cores 1  /usr/hdp/2.6.2.0-205/spark2/examples/jars/spark-examples_2.11-2.1.1.2.6.2.0-205.jar 10

spark = SparkSession \
    .builder \
    .appName("z246390_med_d_parquet_2018_10-02") \
    .config("spark.executor.memory","16GB") \
-	    .config("hive.exec.orc.split.strategy","BI")\        <- or I have tried the other one (.config(“spark.driver.memory”,”8g”) ), didn’t work on helping saving files
    .config("spark.dynamicAllocation.enabled","true") \
    .config("spark.executor.cores","8")\
    .config("spark.shuffle.service.enabled", "true")\
    .config("spark.dynamicAllocation.initialExecutors","10")\
    .config("spark.yarn.executor.memoryOverhead","4096")\
    .config("spark.yarn.queue", "monitoring") \                                                   Change Queue Name
    .config("spark.master","yarn") \
    .config("spark.submit.deployMode","client")\
    .config("spark.dynamicAllocation.executorIdleTimeout","90s")\
    .config("spark.sql.orc.filterPushdown","true") \
    .enableHiveSupport()\
    .getOrCreate()
    
    
    oracle connection string
    “connection_string": "jdbc:oracle:thin:@(DESCRIPTION = (ENABLE=BROKEN)(SDU=8192)(ADDRESS = (PROTOCOL = TCPS)(Host = hostname)(Port = 2484)) (CONNECT_DATA = (SID = HSID)))"
    
    curl -v -u admin:Alph@420 -H "Content-Type: application/json" -H "X-Requested-By:ambari"  -X GET "https://hostnamep:8080/api/v1/views/CAPACITY-SCHEDULER/versions/1.0.0/instances/AUTO_CS_INSTANCE/resources/scheduler" -k
    
    > set hive.tez.container.size=8192;
    > set hive.execution.engine=tez;
    > set hive.vectorized.execution.enabled=true;
    > set hive.vectorized.execution.reduce.enabled=true;
    > set hive.tez.java.opts=-Xmx6553.6m;
    > set tez.runtime.io.sort.mb=3276.8;
    > set tez.runtime.unordered.output.buffer.size-mb=819.2;
    > set hive.tez.container.size=16384;
    > set hive.tez.java.opts=-Xmx14336m;
    > set tez.runtime.io.sort.mb=4750;
    > set tez.runtime.unordered.output.buffer.size-mb=1638.4;
    > set hive.fetch.task.conversion=none;
    > explain select  .................
    
    


